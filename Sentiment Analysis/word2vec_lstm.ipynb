{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors \n",
    "filename = r'D:\\Desktop_folders\\Placement\\resume\\verification\\Sentiment Analysis\\GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded imdb dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))\n",
    "word_index = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "imdb_dataset = pd.Series(X_train).apply(lambda x: [id2word.get(i) for i in x ]) \n",
    "embedding_size=300\n",
    "word2vec_custom = Word2Vec(imdb_dataset, min_count = 1, size = embedding_size, window = 5)\n",
    "word2vec_custom.save('word2vec_custom_imdb.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "def train_lstm(X_train, y_train, embedding_matrix, embedding_trainable, embedding_size=300, batch_size = 64, num_epochs = 3):\n",
    "    embedding_size=300 \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocabulary_size + 1, embedding_size, \n",
    "                        weights=[embedding_matrix], input_length=max_words, \n",
    "                        trainable = embedding_trainable))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    batch_size = 64\n",
    "    num_epochs = 3\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train2, X_valid, y_train2, y_valid = train_test_split(X_train,y_train, test_size = 0.33, random_state = 123)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "print(\"'Review':\",[id2word.get(i, ' ') for i in X_train[0]])\n",
    "print(\"\\n'Label':\",y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum review length: {}'.format(len(max((X_train + X_test), key=len))))\n",
    "print('Minimum review length: {}'.format(len(min((X_train + X_test), key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_words = 1000\n",
    "X_train = pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(word2vec['hi'])\n",
    "word_index = imdb.get_word_index()\n",
    "embedding_matrix = np.zeros((vocabulary_size + 1, embedding_dim))\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "count = 0\n",
    "for i in range(1,vocabulary_size + 1):\n",
    "    word = id2word.get(i)\n",
    "    try: \n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    except:\n",
    "        count +=1     \n",
    "print('No. of words not available in the dictionary:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lstm(X_train, y_train, embedding_matrix = embedding_matrix, embedding_trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lstm(X_train, y_train, embedding_matrix = embedding_matrix, embedding_trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "word2vec_vocab = list(word2vec.vocab.keys())\n",
    "X_removed = pd.Series(X_train).apply(lambda x: [id2word.get(i) for i in x if id2word.get(i) in word2vec_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_imdb = Word2Vec.load('word2vec_custom_imdb.bin')\n",
    "embedding_dim = 300\n",
    "word_index = imdb.get_word_index()\n",
    "embedding_matrix = np.zeros((vocabulary_size + 1, embedding_dim))\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "for i in range(1,vocabulary_size + 1):\n",
    "    word = id2word.get(i)\n",
    "    try: \n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    except:  \n",
    "        try:\n",
    "            embedding_matrix[i] = word2vec_imdb[word]\n",
    "        except:\n",
    "            print('Why is this word not found in imdb dataset?','Index:',i,', Word:',word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lstm(X_train, y_train, embedding_matrix = embedding_matrix, embedding_trainable = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fea11907e4c10c35d5cb464acd3f6a5dd807d5c48e4d5f377ae1b9227492d5a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
