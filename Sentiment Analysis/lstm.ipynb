{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'D:\\Desktop_folders\\Placement\\resume\\verification\\Sentiment Analysis\\imdb_small.csv')\n",
    "map_dict = {'negative':0,'positive':1}\n",
    "train =train.replace({'sentiment':map_dict})  \n",
    "train['review'] = train['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import num2words\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def convertnum2words (sentence):\n",
    "    new_sentence = sentence\n",
    "    for i in sentence.split():\n",
    "        if i.isdigit():\n",
    "            sentence = sentence.replace(i,num2words.num2words(int(i)))\n",
    "    return sentence\n",
    "\n",
    "def countstopwords(sentence):\n",
    "    count = 0\n",
    "    for i in (sentence.split()):\n",
    "        if i in stop_words:\n",
    "            count +=1 \n",
    "    return count\n",
    "    \n",
    "train['review'] = train['review'].apply(lambda x: x.replace('<br />','.'))\n",
    "train['review'] = train['review'].str.replace('[^\\w\\s]','')\n",
    "train['review'] = train['review'].apply(lambda x: convertnum2words(x))\n",
    "train['review'] = train['review'].str.replace('-',' ')\n",
    "train['review'] = train['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['review'] = train['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "train['review'] = train['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_features = 10000 \n",
    "tokenizer = Tokenizer(num_words=max_features, lower = True, split=' ')\n",
    "tokenizer.fit_on_texts(train['review'])\n",
    "x = tokenizer.texts_to_sequences(train['review'])\n",
    "x = pad_sequences(x) \n",
    "word_index = tokenizer.word_index\n",
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "embedding_size=300\n",
    "max_words = len(x[0])\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(x,train['sentiment'], test_size = 0.33, random_state = 123)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size=300\n",
    "max_words = len(x[0])\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(x,train['sentiment'], test_size = 0.33, random_state = 123)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 1000\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "embedding_size=300\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size=300\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train,y_train, test_size = 0.33, random_state = 123)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fea11907e4c10c35d5cb464acd3f6a5dd807d5c48e4d5f377ae1b9227492d5a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
